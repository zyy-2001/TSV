Map:   0%|          | 0/17944 [00:00<?, ? examples/s]Map:   1%|          | 120/17944 [00:00<00:15, 1165.41 examples/s]Map:   1%|▏         | 248/17944 [00:00<00:14, 1224.60 examples/s]Map:   2%|▏         | 377/17944 [00:00<00:14, 1248.97 examples/s]Map:   3%|▎         | 506/17944 [00:00<00:13, 1261.41 examples/s]Map:   4%|▎         | 635/17944 [00:00<00:13, 1269.45 examples/s]Map:   4%|▍         | 765/17944 [00:00<00:13, 1275.03 examples/s]Map:   5%|▍         | 895/17944 [00:00<00:13, 1278.24 examples/s]Map:   6%|▌         | 1024/17944 [00:00<00:13, 1279.98 examples/s]Map:   6%|▋         | 1153/17944 [00:00<00:13, 1279.94 examples/s]Map:   7%|▋         | 1282/17944 [00:01<00:13, 1279.93 examples/s]Map:   8%|▊         | 1412/17944 [00:01<00:12, 1282.13 examples/s]Map:   9%|▊         | 1542/17944 [00:01<00:12, 1283.99 examples/s]Map:   9%|▉         | 1672/17944 [00:01<00:12, 1285.92 examples/s]Map:  10%|█         | 1802/17944 [00:01<00:12, 1287.65 examples/s]Map:  11%|█         | 1932/17944 [00:01<00:12, 1287.81 examples/s]Map:  11%|█▏        | 2062/17944 [00:01<00:12, 1287.97 examples/s]Map:  12%|█▏        | 2192/17944 [00:01<00:12, 1288.21 examples/s]Map:  13%|█▎        | 2321/17944 [00:01<00:12, 1287.86 examples/s]Map:  14%|█▎        | 2450/17944 [00:01<00:12, 1284.66 examples/s]Map:  14%|█▍        | 2580/17944 [00:02<00:11, 1286.93 examples/s]Map:  15%|█▌        | 2773/17944 [00:02<00:11, 1283.23 examples/s]Map:  17%|█▋        | 2964/17944 [00:02<00:11, 1276.94 examples/s]Map:  17%|█▋        | 3093/17944 [00:02<00:11, 1279.13 examples/s]Map:  18%|█▊        | 3223/17944 [00:02<00:11, 1281.96 examples/s]Map:  19%|█▊        | 3352/17944 [00:02<00:11, 1282.61 examples/s]Map:  19%|█▉        | 3482/17944 [00:02<00:11, 1283.76 examples/s]Map:  20%|██        | 3673/17944 [00:02<00:11, 1277.78 examples/s]Map:  21%|██        | 3803/17944 [00:02<00:11, 1280.52 examples/s]Map:  22%|██▏       | 3933/17944 [00:03<00:10, 1282.67 examples/s]Map:  23%|██▎       | 4063/17944 [00:03<00:10, 1284.86 examples/s]Map:  24%|██▎       | 4256/17944 [00:03<00:10, 1282.71 examples/s]Map:  24%|██▍       | 4386/17944 [00:03<00:10, 1284.76 examples/s]Map:  25%|██▌       | 4516/17944 [00:03<00:10, 1286.65 examples/s]Map:  26%|██▌       | 4645/17944 [00:03<00:10, 1284.58 examples/s]Map:  27%|██▋       | 4838/17944 [00:03<00:10, 1282.74 examples/s]Map:  28%|██▊       | 4967/17944 [00:03<00:10, 1281.67 examples/s]Map:  28%|██▊       | 5097/17944 [00:03<00:10, 1283.04 examples/s]Map:  29%|██▉       | 5276/17944 [00:04<00:10, 1247.36 examples/s]Map:  30%|███       | 5406/17944 [00:04<00:09, 1258.82 examples/s]Map:  31%|███       | 5536/17944 [00:04<00:09, 1267.59 examples/s]Map:  32%|███▏      | 5666/17944 [00:04<00:09, 1274.31 examples/s]Map:  32%|███▏      | 5796/17944 [00:04<00:09, 1279.63 examples/s]Map:  33%|███▎      | 5987/17944 [00:04<00:09, 1274.12 examples/s]Map:  34%|███▍      | 6116/17944 [00:04<00:09, 1276.89 examples/s]Map:  35%|███▍      | 6246/17944 [00:04<00:09, 1280.66 examples/s]Map:  36%|███▌      | 6376/17944 [00:04<00:09, 1284.19 examples/s]Map:  36%|███▋      | 6506/17944 [00:05<00:08, 1287.11 examples/s]Map:  37%|███▋      | 6636/17944 [00:05<00:08, 1288.16 examples/s]Map:  38%|███▊      | 6766/17944 [00:05<00:08, 1287.54 examples/s]Map:  39%|███▉      | 6958/17944 [00:05<00:08, 1280.00 examples/s]Map:  39%|███▉      | 7087/17944 [00:05<00:08, 1282.05 examples/s]Map:  41%|████      | 7275/17944 [00:05<00:08, 1268.87 examples/s]Map:  41%|████▏     | 7405/17944 [00:05<00:08, 1274.13 examples/s]Map:  42%|████▏     | 7535/17944 [00:05<00:08, 1278.26 examples/s]Map:  43%|████▎     | 7665/17944 [00:05<00:08, 1282.22 examples/s]Map:  43%|████▎     | 7795/17944 [00:06<00:07, 1285.88 examples/s]Map:  44%|████▍     | 7925/17944 [00:06<00:07, 1288.43 examples/s]Map:  45%|████▌     | 8116/17944 [00:06<00:07, 1278.86 examples/s]Map:  46%|████▌     | 8245/17944 [00:06<00:07, 1280.45 examples/s]Map:  47%|████▋     | 8375/17944 [00:06<00:07, 1283.74 examples/s]Map:  47%|████▋     | 8505/17944 [00:06<00:07, 1286.85 examples/s]Map:  48%|████▊     | 8635/17944 [00:06<00:07, 1288.63 examples/s]Map:  49%|████▉     | 8765/17944 [00:06<00:07, 1289.75 examples/s]Map:  50%|████▉     | 8895/17944 [00:06<00:07, 1290.31 examples/s]Map:  50%|█████     | 9025/17944 [00:07<00:06, 1289.51 examples/s]Map:  51%|█████     | 9155/17944 [00:07<00:06, 1289.60 examples/s]Map:  52%|█████▏    | 9348/17944 [00:07<00:06, 1286.24 examples/s]Map:  53%|█████▎    | 9478/17944 [00:07<00:06, 1287.89 examples/s]Map:  54%|█████▎    | 9608/17944 [00:07<00:06, 1288.42 examples/s]Map:  55%|█████▍    | 9800/17944 [00:07<00:06, 1281.28 examples/s]Map:  55%|█████▌    | 9930/17944 [00:07<00:06, 1282.83 examples/s]Map:  65%|██████▍   | 11605/17944 [00:07<00:01, 5465.62 examples/s]Map:  76%|███████▌  | 13613/17944 [00:07<00:00, 9539.10 examples/s]Map:  86%|████████▋ | 15510/17944 [00:08<00:00, 12224.41 examples/s]Map:  97%|█████████▋| 17465/17944 [00:08<00:00, 14341.82 examples/s]Map: 100%|██████████| 17944/17944 [00:08<00:00, 2142.89 examples/s] 
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:03<00:09,  3.21s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:06<00:06,  3.21s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:09<00:03,  3.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.09s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.13s/it]
  0%|          | 0/9960 [00:00<?, ?it/s]  2%|▏         | 157/9960 [00:00<00:06, 1563.36it/s]  3%|▎         | 325/9960 [00:00<00:05, 1626.85it/s]  5%|▍         | 497/9960 [00:00<00:05, 1665.26it/s]  7%|▋         | 666/9960 [00:00<00:05, 1674.14it/s]  8%|▊         | 836/9960 [00:00<00:05, 1680.69it/s] 10%|█         | 1005/9960 [00:00<00:05, 1682.44it/s] 12%|█▏        | 1174/9960 [00:00<00:05, 1532.95it/s] 14%|█▎        | 1345/9960 [00:00<00:05, 1583.24it/s] 15%|█▌        | 1516/9960 [00:00<00:05, 1619.44it/s] 17%|█▋        | 1686/9960 [00:01<00:05, 1643.08it/s] 19%|█▊        | 1860/9960 [00:01<00:04, 1669.73it/s] 20%|██        | 2028/9960 [00:01<00:04, 1648.04it/s] 22%|██▏       | 2202/9960 [00:01<00:04, 1674.67it/s] 24%|██▍       | 2373/9960 [00:01<00:04, 1682.99it/s] 26%|██▌       | 2543/9960 [00:01<00:04, 1685.39it/s] 27%|██▋       | 2712/9960 [00:02<00:12, 574.94it/s]  29%|██▉       | 2877/9960 [00:02<00:09, 710.78it/s] 31%|███       | 3046/9960 [00:02<00:08, 860.37it/s] 32%|███▏      | 3213/9960 [00:02<00:06, 1005.61it/s] 34%|███▍      | 3385/9960 [00:02<00:05, 1151.00it/s] 36%|███▌      | 3554/9960 [00:02<00:05, 1272.54it/s] 37%|███▋      | 3720/9960 [00:02<00:04, 1365.87it/s] 39%|███▉      | 3885/9960 [00:02<00:04, 1438.95it/s] 41%|████      | 4049/9960 [00:03<00:03, 1487.74it/s] 42%|████▏     | 4212/9960 [00:03<00:03, 1526.06it/s] 44%|████▍     | 4377/9960 [00:03<00:03, 1559.99it/s] 46%|████▌     | 4544/9960 [00:03<00:03, 1590.01it/s] 47%|████▋     | 4711/9960 [00:03<00:03, 1611.92it/s] 49%|████▉     | 4884/9960 [00:03<00:03, 1646.42it/s] 51%|█████     | 5061/9960 [00:03<00:02, 1681.79it/s] 53%|█████▎    | 5236/9960 [00:03<00:02, 1701.17it/s] 54%|█████▍    | 5410/9960 [00:03<00:02, 1712.20it/s] 56%|█████▌    | 5585/9960 [00:03<00:02, 1721.77it/s] 58%|█████▊    | 5758/9960 [00:04<00:02, 1717.52it/s] 60%|█████▉    | 5931/9960 [00:04<00:02, 1718.89it/s] 61%|██████▏   | 6106/9960 [00:04<00:02, 1725.82it/s] 63%|██████▎   | 6280/9960 [00:04<00:02, 1729.32it/s] 65%|██████▍   | 6454/9960 [00:04<00:02, 1729.48it/s] 67%|██████▋   | 6628/9960 [00:05<00:06, 551.62it/s]  68%|██████▊   | 6803/9960 [00:05<00:04, 694.60it/s] 70%|███████   | 6977/9960 [00:05<00:03, 846.53it/s] 72%|███████▏  | 7149/9960 [00:05<00:02, 996.36it/s] 74%|███████▎  | 7321/9960 [00:05<00:02, 1138.98it/s] 75%|███████▌  | 7487/9960 [00:05<00:01, 1253.35it/s] 77%|███████▋  | 7650/9960 [00:05<00:01, 1339.47it/s] 79%|███████▊  | 7822/9960 [00:06<00:01, 1435.67it/s] 80%|████████  | 7993/9960 [00:06<00:01, 1508.19it/s] 82%|████████▏ | 8165/9960 [00:06<00:01, 1566.41it/s] 84%|████████▎ | 8337/9960 [00:06<00:01, 1608.65it/s] 85%|████████▌ | 8509/9960 [00:06<00:00, 1638.65it/s] 87%|████████▋ | 8679/9960 [00:06<00:00, 1654.71it/s] 89%|████████▉ | 8850/9960 [00:06<00:00, 1670.77it/s] 91%|█████████ | 9022/9960 [00:06<00:00, 1683.86it/s] 92%|█████████▏| 9196/9960 [00:06<00:00, 1697.65it/s] 94%|█████████▍| 9372/9960 [00:06<00:00, 1714.56it/s] 96%|█████████▌| 9549/9960 [00:07<00:00, 1729.92it/s] 98%|█████████▊| 9723/9960 [00:07<00:00, 1702.14it/s] 99%|█████████▉| 9894/9960 [00:07<00:00, 1689.14it/s]100%|██████████| 9960/9960 [00:07<00:00, 1370.51it/s]
Epoch 1/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)
Epoch 1/20 Batches: 100%|██████████| 1/1 [00:03<00:00,  3.37s/it]                                                                 epoch_loss: 0.713432788848877
Epoch [1/20], Loss: 0.7134
Best test AUROC: 0.5220, at epoch: 0
Epoch [1/20],Test AUROC: 0.5220
Epoch 2/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 2/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]                                                                 epoch_loss: 0.6901445984840393
Epoch [2/20], Loss: 0.6901
Best test AUROC: 0.6002, at epoch: 1
Epoch [2/20],Test AUROC: 0.6002
Epoch 3/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 3/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]                                                                 epoch_loss: 0.6764426231384277
Epoch [3/20], Loss: 0.6764
Best test AUROC: 0.6537, at epoch: 2
Epoch [3/20],Test AUROC: 0.6537
Epoch 4/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 4/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]                                                                 epoch_loss: 0.6644164323806763
Epoch [4/20], Loss: 0.6644
Best test AUROC: 0.6915, at epoch: 3
Epoch [4/20],Test AUROC: 0.6915
Epoch 5/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 5/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]                                                                 epoch_loss: 0.6525993943214417
Epoch [5/20], Loss: 0.6526
Best test AUROC: 0.7190, at epoch: 4
Epoch [5/20],Test AUROC: 0.7190
Epoch 6/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 6/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]                                                                 epoch_loss: 0.6403971910476685
Epoch [6/20], Loss: 0.6404
Best test AUROC: 0.7399, at epoch: 5
Epoch [6/20],Test AUROC: 0.7399
Epoch 7/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 7/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]                                                                 epoch_loss: 0.6274564266204834
Epoch [7/20], Loss: 0.6275
Best test AUROC: 0.7569, at epoch: 6
Epoch [7/20],Test AUROC: 0.7569
Epoch 8/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 8/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]                                                                 epoch_loss: 0.6133245229721069
Epoch [8/20], Loss: 0.6133
Best test AUROC: 0.7709, at epoch: 7
Epoch [8/20],Test AUROC: 0.7709
Epoch 9/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 9/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]                                                                 epoch_loss: 0.5979816317558289
Epoch [9/20], Loss: 0.5980
Best test AUROC: 0.7812, at epoch: 8
Epoch [9/20],Test AUROC: 0.7812
Epoch 10/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 10/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]                                                                  epoch_loss: 0.5832483768463135
Epoch [10/20], Loss: 0.5832
Best test AUROC: 0.7880, at epoch: 9
Epoch [10/20],Test AUROC: 0.7880
Epoch 11/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 11/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]                                                                  epoch_loss: 0.5680540800094604
Epoch [11/20], Loss: 0.5681
Best test AUROC: 0.7928, at epoch: 10
Epoch [11/20],Test AUROC: 0.7928
Epoch 12/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 12/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]                                                                  epoch_loss: 0.553001880645752
Epoch [12/20], Loss: 0.5530
Best test AUROC: 0.7968, at epoch: 11
Epoch [12/20],Test AUROC: 0.7968
Epoch 13/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 13/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]                                                                  epoch_loss: 0.5374534130096436
Epoch [13/20], Loss: 0.5375
Best test AUROC: 0.8015, at epoch: 12
Epoch [13/20],Test AUROC: 0.8015
Epoch 14/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 14/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]                                                                  epoch_loss: 0.5195775032043457
Epoch [14/20], Loss: 0.5196
Best test AUROC: 0.8059, at epoch: 13
Epoch [14/20],Test AUROC: 0.8059
Epoch 15/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 15/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]                                                                  epoch_loss: 0.5004906058311462
Epoch [15/20], Loss: 0.5005
Best test AUROC: 0.8085, at epoch: 14
Epoch [15/20],Test AUROC: 0.8085
Epoch 16/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 16/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]                                                                  epoch_loss: 0.48097461462020874
Epoch [16/20], Loss: 0.4810
Best test AUROC: 0.8091, at epoch: 15
Epoch [16/20],Test AUROC: 0.8091
Epoch 17/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 17/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]                                                                  epoch_loss: 0.4602304697036743
Epoch [17/20], Loss: 0.4602
Best test AUROC: 0.8100, at epoch: 16
Epoch [17/20],Test AUROC: 0.8100
Epoch 18/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 18/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]                                                                  epoch_loss: 0.43834835290908813
Epoch [18/20], Loss: 0.4383
Best test AUROC: 0.8112, at epoch: 17
Epoch [18/20],Test AUROC: 0.8112
Epoch 19/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 19/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]                                                                  epoch_loss: 0.41463503241539
Epoch [19/20], Loss: 0.4146
Best test AUROC: 0.8113, at epoch: 18
Epoch [19/20],Test AUROC: 0.8113
Epoch 20/20 Batches:   0%|          | 0/1 [00:00<?, ?it/s]Epoch 20/20 Batches: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]                                                                  epoch_loss: 0.3897132873535156
Epoch [20/20], Loss: 0.3897
Epoch [20/20],Test AUROC: 0.8097
  0%|          | 0/115 [00:00<?, ?it/s]  1%|          | 1/115 [00:00<00:54,  2.11it/s]  2%|▏         | 2/115 [00:01<00:57,  1.98it/s]  3%|▎         | 3/115 [00:01<00:47,  2.34it/s]  3%|▎         | 4/115 [00:01<00:51,  2.15it/s]  4%|▍         | 5/115 [00:02<00:52,  2.09it/s]  5%|▌         | 6/115 [00:02<00:49,  2.22it/s]  6%|▌         | 7/115 [00:03<00:45,  2.38it/s]  7%|▋         | 8/115 [00:03<00:44,  2.39it/s]  8%|▊         | 9/115 [00:04<00:46,  2.26it/s]  9%|▊         | 10/115 [00:04<00:49,  2.14it/s] 10%|▉         | 11/115 [00:04<00:44,  2.36it/s] 10%|█         | 12/115 [00:05<00:48,  2.14it/s] 11%|█▏        | 13/115 [00:05<00:48,  2.12it/s] 12%|█▏        | 14/115 [00:06<00:48,  2.07it/s] 13%|█▎        | 15/115 [00:06<00:43,  2.29it/s] 14%|█▍        | 16/115 [00:07<00:47,  2.07it/s] 15%|█▍        | 17/115 [00:07<00:49,  1.99it/s] 16%|█▌        | 18/115 [00:08<00:50,  1.91it/s] 17%|█▋        | 19/115 [00:08<00:44,  2.14it/s] 17%|█▋        | 20/115 [00:09<00:43,  2.21it/s] 18%|█▊        | 21/115 [00:09<00:44,  2.11it/s] 19%|█▉        | 22/115 [00:10<00:44,  2.08it/s] 20%|██        | 23/115 [00:10<00:39,  2.30it/s] 21%|██        | 24/115 [00:10<00:38,  2.36it/s] 22%|██▏       | 25/115 [00:11<00:35,  2.53it/s] 23%|██▎       | 26/115 [00:11<00:33,  2.67it/s] 23%|██▎       | 27/115 [00:12<00:36,  2.38it/s] 24%|██▍       | 28/115 [00:12<00:39,  2.19it/s] 25%|██▌       | 29/115 [00:13<00:44,  1.95it/s] 26%|██▌       | 30/115 [00:13<00:43,  1.94it/s] 27%|██▋       | 31/115 [00:14<00:39,  2.10it/s] 28%|██▊       | 32/115 [00:14<00:39,  2.09it/s] 29%|██▊       | 33/115 [00:15<00:42,  1.95it/s] 30%|██▉       | 34/115 [00:15<00:40,  2.01it/s] 30%|███       | 35/115 [00:16<00:38,  2.07it/s] 31%|███▏      | 36/115 [00:16<00:39,  2.01it/s] 32%|███▏      | 37/115 [00:17<00:40,  1.94it/s] 33%|███▎      | 38/115 [00:17<00:38,  1.99it/s] 34%|███▍      | 39/115 [00:18<00:35,  2.13it/s] 35%|███▍      | 40/115 [00:18<00:34,  2.20it/s] 36%|███▌      | 41/115 [00:19<00:37,  1.99it/s] 37%|███▋      | 42/115 [00:19<00:36,  2.01it/s] 37%|███▋      | 43/115 [00:20<00:41,  1.74it/s] 38%|███▊      | 44/115 [00:23<01:34,  1.33s/it] 39%|███▉      | 45/115 [00:24<01:16,  1.09s/it] 40%|████      | 46/115 [00:24<01:05,  1.06it/s] 41%|████      | 47/115 [00:26<01:25,  1.26s/it] 42%|████▏     | 48/115 [00:27<01:15,  1.13s/it] 43%|████▎     | 49/115 [00:28<01:04,  1.02it/s] 43%|████▎     | 50/115 [00:28<00:55,  1.17it/s] 44%|████▍     | 51/115 [00:29<00:47,  1.34it/s] 45%|████▌     | 52/115 [00:29<00:47,  1.31it/s] 46%|████▌     | 53/115 [00:30<00:44,  1.40it/s] 47%|████▋     | 54/115 [00:31<00:40,  1.50it/s] 48%|████▊     | 55/115 [00:31<00:33,  1.78it/s] 49%|████▊     | 56/115 [00:32<00:49,  1.20it/s] 50%|████▉     | 57/115 [00:33<00:38,  1.50it/s] 50%|█████     | 58/115 [00:33<00:32,  1.74it/s] 51%|█████▏    | 59/115 [00:33<00:27,  2.04it/s] 52%|█████▏    | 60/115 [00:34<00:27,  2.00it/s] 53%|█████▎    | 61/115 [00:36<00:45,  1.18it/s] 54%|█████▍    | 62/115 [00:36<00:39,  1.34it/s] 55%|█████▍    | 63/115 [00:37<00:36,  1.41it/s] 56%|█████▌    | 64/115 [00:37<00:37,  1.37it/s] 57%|█████▋    | 65/115 [00:38<00:32,  1.56it/s] 57%|█████▋    | 66/115 [00:38<00:29,  1.64it/s] 58%|█████▊    | 67/115 [00:39<00:25,  1.87it/s] 59%|█████▉    | 68/115 [00:39<00:23,  2.04it/s] 60%|██████    | 69/115 [00:40<00:22,  2.05it/s] 61%|██████    | 70/115 [00:41<00:32,  1.37it/s] 62%|██████▏   | 71/115 [00:41<00:28,  1.55it/s] 63%|██████▎   | 72/115 [00:42<00:25,  1.67it/s] 63%|██████▎   | 73/115 [00:42<00:22,  1.90it/s] 64%|██████▍   | 74/115 [00:43<00:19,  2.07it/s] 65%|██████▌   | 75/115 [00:45<00:36,  1.10it/s] 66%|██████▌   | 76/115 [00:45<00:31,  1.25it/s] 67%|██████▋   | 77/115 [00:46<00:27,  1.39it/s] 68%|██████▊   | 78/115 [00:46<00:22,  1.63it/s] 69%|██████▊   | 79/115 [00:46<00:21,  1.71it/s] 70%|██████▉   | 80/115 [00:48<00:26,  1.34it/s] 70%|███████   | 81/115 [00:48<00:21,  1.56it/s] 71%|███████▏  | 82/115 [00:48<00:18,  1.74it/s] 72%|███████▏  | 83/115 [00:49<00:16,  1.94it/s] 73%|███████▎  | 84/115 [00:49<00:15,  1.96it/s] 74%|███████▍  | 85/115 [00:50<00:14,  2.01it/s] 75%|███████▍  | 86/115 [00:50<00:14,  1.97it/s] 76%|███████▌  | 87/115 [00:51<00:15,  1.81it/s] 77%|███████▋  | 88/115 [00:51<00:14,  1.88it/s] 77%|███████▋  | 89/115 [00:52<00:11,  2.17it/s] 78%|███████▊  | 90/115 [00:54<00:23,  1.05it/s] 79%|███████▉  | 91/115 [00:55<00:20,  1.15it/s] 80%|████████  | 92/115 [00:55<00:18,  1.26it/s] 81%|████████  | 93/115 [00:56<00:15,  1.39it/s] 82%|████████▏ | 94/115 [00:57<00:18,  1.11it/s] 83%|████████▎ | 95/115 [00:57<00:15,  1.29it/s] 83%|████████▎ | 96/115 [00:58<00:13,  1.38it/s] 84%|████████▍ | 97/115 [00:59<00:11,  1.54it/s] 85%|████████▌ | 98/115 [00:59<00:09,  1.76it/s] 86%|████████▌ | 99/115 [01:00<00:12,  1.31it/s] 87%|████████▋ | 100/115 [01:00<00:09,  1.57it/s] 88%|████████▊ | 101/115 [01:01<00:08,  1.69it/s] 89%|████████▊ | 102/115 [01:03<00:13,  1.02s/it] 90%|████████▉ | 103/115 [01:03<00:09,  1.24it/s] 90%|█████████ | 104/115 [01:04<00:07,  1.42it/s] 91%|█████████▏| 105/115 [01:04<00:05,  1.75it/s] 92%|█████████▏| 106/115 [01:06<00:09,  1.06s/it] 93%|█████████▎| 107/115 [01:07<00:06,  1.17it/s] 94%|█████████▍| 108/115 [01:07<00:04,  1.46it/s] 95%|█████████▍| 109/115 [01:07<00:03,  1.74it/s] 96%|█████████▌| 110/115 [01:09<00:04,  1.08it/s] 97%|█████████▋| 111/115 [01:10<00:03,  1.15it/s] 97%|█████████▋| 112/115 [01:12<00:04,  1.34s/it] 98%|█████████▊| 113/115 [01:13<00:02,  1.06s/it] 99%|█████████▉| 114/115 [01:13<00:00,  1.14it/s]100%|██████████| 115/115 [01:13<00:00,  1.56it/s]
tsv_main2.py:175: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  exemplar_label = torch.tensor(exemplar_labels).cuda()
tsv_main2.py:182: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  augmented_labels = torch.concat((selected_labels, torch.tensor(exemplar_labels).clone().cuda()))
Epoch 1/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 1/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.14it/s]Epoch 1/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.37it/s]Epoch 1/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.23it/s]                                                                 epoch_loss: 0.28771207729975384
Epoch [1/20], Loss: 0.2877
Epoch 2/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 2/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.13it/s]Epoch 2/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.37it/s]Epoch 2/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                 epoch_loss: 0.2315673977136612
Epoch [2/20], Loss: 0.2316
Epoch 3/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 3/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Epoch 3/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 3/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                 epoch_loss: 0.19728724161783853
Epoch [3/20], Loss: 0.1973
Epoch 4/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 4/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.13it/s]Epoch 4/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 4/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                 epoch_loss: 0.16163485497236252
Epoch [4/20], Loss: 0.1616
Epoch 5/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 5/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.13it/s]Epoch 5/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.37it/s]Epoch 5/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                 epoch_loss: 0.13004797200361887
Epoch [5/20], Loss: 0.1300
Epoch 6/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 6/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Epoch 6/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 6/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                 epoch_loss: 0.10420873016119003
Epoch [6/20], Loss: 0.1042
Best test AUROC: 0.8135, at epoch: 25
Epoch 7/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 7/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.13it/s]Epoch 7/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 7/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                 epoch_loss: 0.08389002084732056
Epoch [7/20], Loss: 0.0839
Epoch 8/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 8/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.13it/s]Epoch 8/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 8/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                 epoch_loss: 0.06948907176653545
Epoch [8/20], Loss: 0.0695
Epoch 9/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 9/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.13it/s]Epoch 9/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 9/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                 epoch_loss: 0.05432659635941187
Epoch [9/20], Loss: 0.0543
Epoch 10/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 10/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.13it/s]Epoch 10/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 10/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                  epoch_loss: 0.04430174579222997
Epoch [10/20], Loss: 0.0443
Epoch 11/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 11/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.13it/s]Epoch 11/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 11/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                  epoch_loss: 0.035375590125719704
Epoch [11/20], Loss: 0.0354
Epoch 12/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 12/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Epoch 12/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 12/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                  epoch_loss: 0.0280755665153265
Epoch [12/20], Loss: 0.0281
Epoch 13/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 13/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Epoch 13/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 13/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                  epoch_loss: 0.02270677499473095
Epoch [13/20], Loss: 0.0227
Epoch 14/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 14/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Epoch 14/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 14/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.21it/s]                                                                  epoch_loss: 0.01848521549254656
Epoch [14/20], Loss: 0.0185
Epoch 15/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 15/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Epoch 15/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 15/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.21it/s]                                                                  epoch_loss: 0.015006817256410917
Epoch [15/20], Loss: 0.0150
Epoch 16/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 16/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Epoch 16/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.35it/s]Epoch 16/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.21it/s]                                                                  epoch_loss: 0.012224910780787468
Epoch [16/20], Loss: 0.0122
Epoch 17/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 17/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Epoch 17/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 17/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                  epoch_loss: 0.010026484727859497
Epoch [17/20], Loss: 0.0100
Epoch 18/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 18/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Epoch 18/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 18/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                  epoch_loss: 0.008271001434574524
Epoch [18/20], Loss: 0.0083
Epoch 19/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 19/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Epoch 19/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 19/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                  epoch_loss: 0.006860745760301749
Epoch [19/20], Loss: 0.0069
Epoch 20/20 Batches:   0%|          | 0/3 [00:00<?, ?it/s]/data/zyy/LLM/tsv/train_utils.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  labels = torch.tensor(labels, dtype=torch.long, device=device)
Epoch 20/20 Batches:  33%|███▎      | 1/3 [00:00<00:01,  1.12it/s]Epoch 20/20 Batches:  67%|██████▋   | 2/3 [00:01<00:00,  1.36it/s]Epoch 20/20 Batches: 100%|██████████| 3/3 [00:02<00:00,  1.22it/s]                                                                  epoch_loss: 0.005709689110517502
Epoch [20/20], Loss: 0.0057
best_test_auroc: 0.813520400067101
